{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from skimage.color import lab2rgb, ycbcr2rgb\n",
    "from fastai.vision.learner import create_body\n",
    "from torchvision.models.resnet import resnet18\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from importlib import reload\n",
    "import loss\n",
    "import data_generator\n",
    "import model\n",
    "import train\n",
    "import visualize\n",
    "reload(loss)\n",
    "reload(visualize)\n",
    "reload(model)\n",
    "reload(data_generator)\n",
    "reload(train)\n",
    "\n",
    "from data_generator import make_dataloaders\n",
    "from model import MainModel\n",
    "from train import train_model, load_model, build_res_unet, pretrain_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 200\n"
     ]
    }
   ],
   "source": [
    "COLOR_SPACE = 'Lab' # Lab or HSL or YCbCr\n",
    "path = \"./data/part1\"\n",
    "model_path = f\"./models/model_cropped_faces_{COLOR_SPACE}.pt\"\n",
    "paths = glob.glob(path + \"/*.jpg\") # Grabbing all the image file names\n",
    "np.random.seed(123)\n",
    "paths_subset = np.random.choice(paths, 1000, replace=False) # choosing 1000 images randomly\n",
    "rand_idxs = np.random.permutation(1000)\n",
    "train_idxs = rand_idxs[:800] # choosing the first 8000 as training set\n",
    "val_idxs = rand_idxs[800:] # choosing last 2000 as validation set\n",
    "train_paths = paths_subset[train_idxs]\n",
    "val_paths = paths_subset[val_idxs]\n",
    "print(len(train_paths), len(val_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for ax, img_path in zip(axes.flatten(), train_paths):\n",
    "    ax.imshow(Image.open(img_path))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 256, 256]) torch.Size([8, 2, 256, 256])\n",
      "100 25\n"
     ]
    }
   ],
   "source": [
    "train_dl = make_dataloaders(paths=train_paths, split='train', color_space=COLOR_SPACE)\n",
    "val_dl = make_dataloaders(paths=val_paths, split='val', color_space=COLOR_SPACE)\n",
    "\n",
    "data = next(iter(train_dl))\n",
    "known_channels, unknown_channels_ = data['known_channel'], data['unknown_channels']\n",
    "print(known_channels.shape, unknown_channels_.shape)\n",
    "print(len(train_dl), len(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15d3128e74c4b7bb14aa921b0e31177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n",
      "L1 Loss: 0.08785\n"
     ]
    }
   ],
   "source": [
    "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
    "opt = torch.optim.Adam(net_G.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.L1Loss()        \n",
    "pretrain_generator(net_G, train_dl, opt, criterion, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_G.state_dict(), \"res18-unet.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_G = build_res_unet(n_input=1, n_output=2, size=256)\n",
    "net_G.load_state_dict(torch.load(\"res18-unet.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MainModel(net_G=net_G)\n",
    "train_model(model, train_dl, val_dl, COLOR_SPACE, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import visualize\n",
    "\n",
    "loaded_model = MainModel()\n",
    "_, loss_meter_dict = load_model(model_path, loaded_model)\n",
    "for i, data in enumerate(iter(val_dl)):\n",
    "    visualize(loaded_model, data, COLOR_SPACE, save=False)\n",
    "    if i == 4:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
